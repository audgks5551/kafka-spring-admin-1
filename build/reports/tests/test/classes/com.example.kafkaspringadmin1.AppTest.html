<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - AppTest</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>AppTest</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/com.example.kafkaspringadmin1.html">com.example.kafkaspringadmin1</a> &gt; AppTest</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">1</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">20.308s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">토픽에_데이터_하나_비동기전송()</td>
<td class="success">20.308s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>15:52:02.366 [Test worker] DEBUG org.springframework.test.context.BootstrapUtils - Instantiating CacheAwareContextLoaderDelegate from class [org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate]
15:52:02.373 [Test worker] DEBUG org.springframework.test.context.BootstrapUtils - Instantiating BootstrapContext using constructor [public org.springframework.test.context.support.DefaultBootstrapContext(java.lang.Class,org.springframework.test.context.CacheAwareContextLoaderDelegate)]
15:52:02.381 [Test worker] DEBUG org.springframework.test.context.BootstrapUtils - Instantiating TestContextBootstrapper for test class [com.example.kafkaspringadmin1.AppTest] from class [org.springframework.boot.test.context.SpringBootTestContextBootstrapper]
15:52:02.389 [Test worker] INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Neither @ContextConfiguration nor @ContextHierarchy found for test class [com.example.kafkaspringadmin1.AppTest], using SpringBootContextLoader
15:52:02.392 [Test worker] DEBUG org.springframework.test.context.support.AbstractContextLoader - Did not detect default resource location for test class [com.example.kafkaspringadmin1.AppTest]: class path resource [com/example/kafkaspringadmin1/AppTest-context.xml] does not exist
15:52:02.392 [Test worker] DEBUG org.springframework.test.context.support.AbstractContextLoader - Did not detect default resource location for test class [com.example.kafkaspringadmin1.AppTest]: class path resource [com/example/kafkaspringadmin1/AppTestContext.groovy] does not exist
15:52:02.392 [Test worker] INFO org.springframework.test.context.support.AbstractContextLoader - Could not detect default resource locations for test class [com.example.kafkaspringadmin1.AppTest]: no resource found for suffixes {-context.xml, Context.groovy}.
15:52:02.393 [Test worker] INFO org.springframework.test.context.support.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.example.kafkaspringadmin1.AppTest]: AppTest does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
15:52:02.424 [Test worker] DEBUG org.springframework.test.context.support.ActiveProfilesUtils - Could not find an 'annotation declaring class' for annotation type [org.springframework.test.context.ActiveProfiles] and class [com.example.kafkaspringadmin1.AppTest]
15:52:02.464 [Test worker] DEBUG org.springframework.context.annotation.ClassPathScanningCandidateComponentProvider - Identified candidate component class: file [/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/classes/java/main/com/example/kafkaspringadmin1/KafkaSpringAdmin1Application.class]
15:52:02.464 [Test worker] INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.example.kafkaspringadmin1.KafkaSpringAdmin1Application for test class com.example.kafkaspringadmin1.AppTest
15:52:02.521 [Test worker] DEBUG org.springframework.boot.test.context.SpringBootTestContextBootstrapper - @TestExecutionListeners is not present for class [com.example.kafkaspringadmin1.AppTest]: using defaults.
15:52:02.521 [Test worker] INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Loaded default TestExecutionListener class names from location [META-INF/spring.factories]: [org.springframework.boot.test.autoconfigure.restdocs.RestDocsTestExecutionListener, org.springframework.boot.test.autoconfigure.web.client.MockRestServiceServerResetTestExecutionListener, org.springframework.boot.test.autoconfigure.web.servlet.MockMvcPrintOnlyOnFailureTestExecutionListener, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverTestExecutionListener, org.springframework.boot.test.autoconfigure.webservices.client.MockWebServiceServerTestExecutionListener, org.springframework.boot.test.mock.mockito.MockitoTestExecutionListener, org.springframework.boot.test.mock.mockito.ResetMocksTestExecutionListener, org.springframework.test.context.web.ServletTestExecutionListener, org.springframework.test.context.support.DirtiesContextBeforeModesTestExecutionListener, org.springframework.test.context.event.ApplicationEventsTestExecutionListener, org.springframework.test.context.support.DependencyInjectionTestExecutionListener, org.springframework.test.context.support.DirtiesContextTestExecutionListener, org.springframework.test.context.transaction.TransactionalTestExecutionListener, org.springframework.test.context.jdbc.SqlScriptsTestExecutionListener, org.springframework.test.context.event.EventPublishingTestExecutionListener]
15:52:02.526 [Test worker] DEBUG org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Skipping candidate TestExecutionListener [org.springframework.test.context.web.ServletTestExecutionListener] due to a missing dependency. Specify custom listener classes or make the default listener classes and their required dependencies available. Offending class: [javax/servlet/ServletContext]
15:52:02.530 [Test worker] INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Using TestExecutionListeners: [org.springframework.test.context.support.DirtiesContextBeforeModesTestExecutionListener@3f446bef, org.springframework.test.context.event.ApplicationEventsTestExecutionListener@7829b776, org.springframework.boot.test.mock.mockito.MockitoTestExecutionListener@5778826f, org.springframework.boot.test.autoconfigure.SpringBootDependencyInjectionTestExecutionListener@5b64c4b7, org.springframework.test.context.support.DirtiesContextTestExecutionListener@4763c727, org.springframework.test.context.transaction.TransactionalTestExecutionListener@72445aba, org.springframework.test.context.jdbc.SqlScriptsTestExecutionListener@61bcd567, org.springframework.test.context.event.EventPublishingTestExecutionListener@1c80e49b, org.springframework.boot.test.autoconfigure.restdocs.RestDocsTestExecutionListener@458342d3, org.springframework.boot.test.autoconfigure.web.client.MockRestServiceServerResetTestExecutionListener@15c25153, org.springframework.boot.test.autoconfigure.web.servlet.MockMvcPrintOnlyOnFailureTestExecutionListener@1252b961, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverTestExecutionListener@9ed238c, org.springframework.boot.test.autoconfigure.webservices.client.MockWebServiceServerTestExecutionListener@56276db8, org.springframework.boot.test.mock.mockito.ResetMocksTestExecutionListener@51e8e6e6]
15:52:02.532 [Test worker] DEBUG org.springframework.test.context.support.AbstractDirtiesContextTestExecutionListener - Before test class: context [DefaultTestContext@5b080f3a testClass = AppTest, testInstance = [null], testMethod = [null], testException = [null], mergedContextConfiguration = [MergedContextConfiguration@773cbf4f testClass = AppTest, locations = '{}', classes = '{class com.example.kafkaspringadmin1.KafkaSpringAdmin1Application}', contextInitializerClasses = '[]', activeProfiles = '{}', propertySourceLocations = '{}', propertySourceProperties = '{org.springframework.boot.test.context.SpringBootTestContextBootstrapper=true}', contextCustomizers = set[org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer@f62758e4, org.springframework.boot.test.autoconfigure.actuate.metrics.MetricsExportContextCustomizerFactory$DisableMetricExportContextCustomizer@54e7df6a, org.springframework.boot.test.autoconfigure.properties.PropertyMappingContextCustomizer@0, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverContextCustomizerFactory$Customizer@7331196b, org.springframework.boot.test.context.filter.ExcludeFilterContextCustomizer@e70f13a, org.springframework.boot.test.json.DuplicateJsonObjectContextCustomizerFactory$DuplicateJsonObjectContextCustomizer@d62fe5b, org.springframework.boot.test.mock.mockito.MockitoContextCustomizer@0, org.springframework.boot.test.web.client.TestRestTemplateContextCustomizer@5812f68b, org.springframework.boot.test.context.SpringBootTestArgs@1, org.springframework.boot.test.context.SpringBootTestWebEnvironment@568ff82], contextLoader = 'org.springframework.boot.test.context.SpringBootContextLoader', parent = [null]], attributes = map[[empty]]], class annotated with @DirtiesContext [false] with mode [null].
15:52:02.542 [Test worker] DEBUG org.springframework.test.context.support.DependencyInjectionTestExecutionListener - Performing dependency injection for test context [[DefaultTestContext@5b080f3a testClass = AppTest, testInstance = com.example.kafkaspringadmin1.AppTest@533377b, testMethod = [null], testException = [null], mergedContextConfiguration = [MergedContextConfiguration@773cbf4f testClass = AppTest, locations = '{}', classes = '{class com.example.kafkaspringadmin1.KafkaSpringAdmin1Application}', contextInitializerClasses = '[]', activeProfiles = '{}', propertySourceLocations = '{}', propertySourceProperties = '{org.springframework.boot.test.context.SpringBootTestContextBootstrapper=true}', contextCustomizers = set[org.springframework.kafka.test.context.EmbeddedKafkaContextCustomizer@f62758e4, org.springframework.boot.test.autoconfigure.actuate.metrics.MetricsExportContextCustomizerFactory$DisableMetricExportContextCustomizer@54e7df6a, org.springframework.boot.test.autoconfigure.properties.PropertyMappingContextCustomizer@0, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverContextCustomizerFactory$Customizer@7331196b, org.springframework.boot.test.context.filter.ExcludeFilterContextCustomizer@e70f13a, org.springframework.boot.test.json.DuplicateJsonObjectContextCustomizerFactory$DuplicateJsonObjectContextCustomizer@d62fe5b, org.springframework.boot.test.mock.mockito.MockitoContextCustomizer@0, org.springframework.boot.test.web.client.TestRestTemplateContextCustomizer@5812f68b, org.springframework.boot.test.context.SpringBootTestArgs@1, org.springframework.boot.test.context.SpringBootTestWebEnvironment@568ff82], contextLoader = 'org.springframework.boot.test.context.SpringBootContextLoader', parent = [null]], attributes = map['org.springframework.test.context.event.ApplicationEventsTestExecutionListener.recordApplicationEvents' -&gt; false]]].

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.1)

2022-07-18 15:52:02.858  INFO 13281 --- [    Test worker] k.utils.Log4jControllerRegistration$     : Registered kafka:type=kafka.Log4jController MBean
2022-07-18 15:52:02.868  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : 
2022-07-18 15:52:02.868  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :   ______                  _                                          
2022-07-18 15:52:02.868  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :  |___  /                 | |                                         
2022-07-18 15:52:02.868  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2022-07-18 15:52:02.869  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2022-07-18 15:52:02.869  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :   / /__  | (_) | | (_) | |   &lt;  |  __/ |  __/ | |_) | |  __/ | |    
2022-07-18 15:52:02.869  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2022-07-18 15:52:02.869  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :                                               | |                     
2022-07-18 15:52:02.869  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :                                               |_|                     
2022-07-18 15:52:02.869  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : 
2022-07-18 15:52:02.871  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2022-07-18 15:52:02.871  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:host.name=192.168.0.10
2022-07-18 15:52:02.871  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.version=17.0.3
2022-07-18 15:52:02.871  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.vendor=Amazon.com Inc.
2022-07-18 15:52:02.872  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.home=/Users/myunghan/Library/Java/JavaVirtualMachines/corretto-17.0.3/Contents/Home
2022-07-18 15:52:02.872  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.class.path=/Users/myunghan/.gradle/caches/7.4.1/workerMain/gradle-worker.jar:/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/classes/java/test:/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/classes/java/main:/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/resources/main:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/2.7.1/28ba2633ff51cabd9792ead85cbbffecdbf9f22e/spring-boot-starter-test-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/2.7.1/48f7e04459ccc16d3532bfc486c1b6d629e6e0fc/spring-boot-starter-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/2.8.7/b767af6592e93e0b1a8e34a405f820c778dc7bcf/spring-kafka-2.8.7.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/2.8.7/32bc4c1071fc8bf649d06319afcd3c4d0058e517/spring-kafka-test-2.8.7.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/2.7.1/5b2bed142ba0354200825673413960404a1914e4/spring-boot-test-autoconfigure-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/2.7.1/923ad789b004e8cc17d67853b1e4d3db11946f0/spring-boot-autoconfigure-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/2.7.1/ad38d8a7caa40a499168b2c2e6063fe9b1656373/spring-boot-test-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/2.7.1/8e49b8e7e9ea470a7772f489532264732ab206a2/spring-boot-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/2.7.1/461cf82dc10505f47d3ce2146bd01721177cde4a/spring-boot-starter-logging-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/1.3.5/59eb84ee0d616332ff44aba065f3888cf002cd2d/jakarta.annotation-api-1.3.5.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/5.3.21/fe371c85f02b8c6690fc3b3d0950ef4f965db0cd/spring-context-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/5.3.21/2012d940f72c8d3ccb97e8bc99e642f522659374/spring-messaging-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/5.3.21/13f4f564024d2f85502c151942307c3ca851a4f7/spring-tx-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/5.3.21/77f04111344acf6ae422aacacb279819ccef4d70/spring-test-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/5.3.21/58ec4ff7a0ce30a1e2612f04ad0fb13ea806705/spring-aop-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/5.3.21/e3eae7e6d211381642a0b7507a5215e3ac1b32e1/spring-beans-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/5.3.21/ca8c5822fc528066ec717f1e74160a1575c43192/spring-expression-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/5.3.21/1b0c9be6b972e4c615f175c70fc32e80557e68e8/spring-core-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.30/8fde7fe2586328ac3c68db92045e1c8759125000/snakeyaml-1.30.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.3.3/13a6f4edb1f5a8956ec6aa867757e325bc98eee7/spring-retry-1.3.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.1/3b713a205faca52c8e90ac0191df8f1d32be00e2/kafka_2.13-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.1/cf608d26e9ab86100d4bbef207828aa919e5de5a/kafka_2.13-3.1.1-test.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.1.1/64f32bf8cdf4ea34a7692d11ad1848932f008044/kafka-metadata-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.1.1/523510a99ecc1e1f722eaade9e0f9913d641aded/kafka-streams-test-utils-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.1.1/c9fcde2d63ba3f5cac7e4d529706c33799899a7a/kafka-raft-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.1.1/cdda958a7405af8fd2bbcce0b0b6f09a7aaa5ade/kafka-storage-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.1.1/d95f73a26a7a864bfabd9c350c43e92cb7347594/kafka-server-common-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.1.1/6ecc0a3857815027e053662d6bf1e765ef63464c/kafka-streams-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.1.1/8f352621795f711c5073aece6153548859ea6092/kafka-storage-api-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.1/6fb84c57d8c4a87b3fc1e834e2500a36c048afa3/kafka-clients-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.1/b63bf4f2387f3b37e5a7fc86958343f52c4aac15/kafka-clients-3.1.1-test.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.google.code.findbugs/jsr305/3.0.2/25ea2e8b0c338a877313bd4672d3fe056ea78f0d/jsr305-3.0.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.7.0/f9d7d9659f2694e61142046ff8a216c047f263e8/json-path-2.7.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/2.3.3/48e3b9cfc10752fba3521d6511f4165bea951801/jakarta.xml.bind-api-2.3.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.22.0/c300c0c6a24559f35fa0bd3a5472dc1edcd0111e/assertj-core-3.22.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/4.5.1/f81fb60bd69b3a6e5537ae23b883326f01632a61/mockito-junit-jupiter-4.5.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.8.2/ddeafe92fc263f895bfb73ffeca7fd56e23c2cce/junit-jupiter-params-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.8.2/c598b4328d2f397194d11df3b1648d68d7d990e3/junit-jupiter-engine-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.8.2/4c21029217adf07e4c0d0c5e192b6bf610c94bdc/junit-jupiter-api-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.8.2/b737de09f19864bd136805c84df7999a142fec29/junit-platform-engine-1.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.8.2/32c8b8617c1342376fd5af2053da6410d8866861/junit-platform-commons-1.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.8.2/5a817b1e63f1217e5c586090c45e681281f097ad/junit-jupiter-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/4.5.1/ed456e623e5afc6f4cee3ae58144e5c45f3b3bf/mockito-core-4.5.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.0/6c9d5fe2f59da598d9aefc1cfc6528ff3cf32df3/jsonassert-1.5.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.9.0/8959725d90eecfee28acd7110e2bb8460285d876/xmlunit-core-2.9.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.6.3/a6e74f826db85ff8c51c15ef0fa2ea0b462aef25/zookeeper-3.6.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.11/4741689214e9d1e8408b206506cbe76d1c6a7d60/logback-classic-1.2.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.17.2/17dd0fae2747d9a28c67bc9534108823d2376b46/log4j-to-slf4j-2.17.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/1.7.36/ed46d81cef9c412a88caef405b58f93a678ff2ca/jul-to-slf4j-1.7.36.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/5.3.21/b41a2888c0e708f9fd12cf9cc0c29cebbcab2e5e/spring-jcl-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.0-4/338d83645fb93afc9e8b38a12d9d16d41d0819b3/zstd-jni-1.5.0-4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.8.4/66f0d56454509f6e36175f2331572e250e04a6cc/snappy-java-1.1.8.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.7.8/34b47db4364d1916c78c3e26e419e8acbff57d80/jose4j-0.7.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.3/d97c3bcdcf6179e110af8ad2a64ca276843395c1/scala-logging_2.13-3.9.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.2.10/4b21499702e50f8be51d8c277ef5bc7d228999ec/metrics-core-4.2.10.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.36/6c62681a2f655b49963a5983b8b0950a6120ae14/slf4j-api-1.7.36.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.4.8/7c62f5f72ab05eb54d40e2abf0360a2fe9ea477f/json-smart-2.4.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/1.2.2/99f53adba383cb1bf7c3862844488574b559621f/jakarta.activation-api-1.2.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.12.11/ebeebf10ea48f997da583cdb6ce861eee279a507/byte-buddy-1.12.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.12.11/78439af17fa17e4336242d1c4a6aa36282c89139/byte-buddy-agent-1.12.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.2/7fadf57620c8b8abdf7519533e5527367cb51f09/objenesis-3.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.6.3/8990d19ec3db01f45f82d4011a11b085db66de05/zookeeper-jute-3.6.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.5.0/55762d3191a8d6610ef46d11e8cb70c7667342a3/audience-annotations-0.5.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.78.Final/7e902b6018378bb700ec9364b1d0fba6eefd99fd/netty-handler-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.78.Final/dcb28ed02abf7bf02aa8dc9bf05f58bedcb12e9a/netty-transport-native-epoll-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.13.3/ad638e951639b8f5eb0c08e5d3c17abdc103e98e/jackson-dataformat-csv-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.13.3/4ad8fb9d69de4a7010ddb16825d6a9de57b2027c/jackson-module-scala_2.13-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.13.3/7198b3aac15285a49e218e08441c5f70af00fc51/jackson-annotations-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.13.3/a27014716e4421684416e5fa83d896ddb87002da/jackson-core-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.13.3/d4884595d5aab5babdb00ddbd693b8fd36b5ec3c/jackson-datatype-jdk8-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.13.3/56deb9ea2c93a7a556b3afbedd616d342963464e/jackson-databind-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.4.4/9cb262981ba1fac1f25c0e7e2b285d536ec8923b/scala-collection-compat_2.13-2.4.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.0/8ffac68615b438933fe27506e755d790a68b8bab/scala-java8-compat_2.13-1.0.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.6/bb523e56c63746a5752dece28fcd702c54fd3a11/scala-reflect-2.13.6.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.6/ed7a2f528c7389ea65746c22a01031613d98ab3d/scala-library-2.13.6.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.2.0/28c11eb91f9b6d8e200631d46e20a7f407f2a046/opentest4j-1.2.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.2.11/a01230df5ca5c34540cdaa3ad5efb012f1f1f792/logback-core-1.2.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.17.2/f42d6afa111b4dec5d2aea0fe2197240749a4ea6/log4j-api-2.17.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.4.8/6e1bee5a530caba91893604d6ab41d0edcecca9a/accessors-smart-2.4.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.78.Final/2151893a521c6362858071053fddbd270c1f20bd/netty-transport-classes-epoll-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.78.Final/62c64e182a11e31ad80e68a94f0b02b45344df23/netty-transport-native-unix-common-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.78.Final/e8486c8923fc0914df4562a8e0923462e885f88a/netty-codec-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.78.Final/b1639d431e43622d6cbfdd45c30d3fb810fa9101/netty-transport-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.78.Final/691170d70a979757d50f60e16121ced5f411a9b8/netty-resolver-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.78.Final/6c0c2d805a2c8ea30223677d8219235f9ec14c38/netty-buffer-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.78.Final/d6f560098050f88ba11750aa856edd955e4a7707/netty-common-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/6.22.1.1/c9cdf28e714bc93a3e7b6c57d583d3508568a606/rocksdbjni-6.22.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.1/a99500cf6eea30535eeac6be73899d048f8d12a8/asm-9.1.jar
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.library.path=/Users/myunghan/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.io.tmpdir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.compiler=&lt;NA&gt;
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.name=Mac OS X
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.arch=x86_64
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.version=12.4
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.name=myunghan
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.home=/Users/myunghan
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.dir=/Users/myunghan/kafka_workspace/kafka-spring-admin-1
2022-07-18 15:52:02.873  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.free=472MB
2022-07-18 15:52:02.874  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.max=512MB
2022-07-18 15:52:02.874  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.total=512MB
2022-07-18 15:52:02.874  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.enableEagerACLCheck = false
2022-07-18 15:52:02.875  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.digest.enabled = true
2022-07-18 15:52:02.875  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.closeSessionTxn.enabled = true
2022-07-18 15:52:02.875  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.flushDelay=0
2022-07-18 15:52:02.875  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.maxWriteQueuePollTime=0
2022-07-18 15:52:02.875  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.maxBatchSize=1000
2022-07-18 15:52:02.875  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.intBufferStartingSizeBytes = 1024
2022-07-18 15:52:02.877  INFO 13281 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : zookeeper.snapshot.trust.empty : false
2022-07-18 15:52:02.889  INFO 13281 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2022-07-18 15:52:02.889  INFO 13281 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2022-07-18 15:52:02.890  INFO 13281 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.snapshotSizeFactor = 0.33
2022-07-18 15:52:02.890  INFO 13281 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.commitLogCount=500
2022-07-18 15:52:02.892  INFO 13281 --- [    Test worker] o.apache.zookeeper.server.BlueThrottle   : Weighed connection throttling is disabled
2022-07-18 15:52:02.895  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : minSessionTimeout set to 1600
2022-07-18 15:52:02.896  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : maxSessionTimeout set to 16000
2022-07-18 15:52:02.896  INFO 13281 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : Response cache size is initialized with value 400.
2022-07-18 15:52:02.897  INFO 13281 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : Response cache size is initialized with value 400.
2022-07-18 15:52:02.898  INFO 13281 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotCapacity = 60
2022-07-18 15:52:02.898  INFO 13281 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotDuration = 15
2022-07-18 15:52:02.898  INFO 13281 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.maxDepth = 6
2022-07-18 15:52:02.899  INFO 13281 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.initialDelay = 5
2022-07-18 15:52:02.899  INFO 13281 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.delay = 5
2022-07-18 15:52:02.899  INFO 13281 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.enabled = false
2022-07-18 15:52:02.901  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The max bytes for all large requests are set to 104857600
2022-07-18 15:52:02.902  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The large request threshold is set to -1
2022-07-18 15:52:02.902  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/kafka-15241940347362370266/version-2 snapdir /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/kafka-5535609423272361441/version-2
2022-07-18 15:52:02.907  WARN 13281 --- [    Test worker] o.a.zookeeper.server.ServerCnxnFactory   : maxCnxns is not configured, using default value 0.
2022-07-18 15:52:02.908  INFO 13281 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 12 worker threads, and 64 kB direct buffers.
2022-07-18 15:52:02.912  INFO 13281 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : binding to port /127.0.0.1:0
2022-07-18 15:52:02.932  INFO 13281 --- [    Test worker] o.a.z.server.persistence.SnapStream      : zookeeper.snapshot.compression.method = CHECKED
2022-07-18 15:52:02.933  INFO 13281 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/kafka-5535609423272361441/version-2/snapshot.0
2022-07-18 15:52:02.937  INFO 13281 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : Snapshot loaded in 23 ms, highest zxid is 0x0, digest is 1371985504
2022-07-18 15:52:02.937  INFO 13281 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/kafka-5535609423272361441/version-2/snapshot.0
2022-07-18 15:52:02.937  INFO 13281 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Snapshot taken in 0 ms
2022-07-18 15:52:02.954  INFO 13281 --- [0 cport:59528):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2022-07-18 15:52:02.954  INFO 13281 --- [    Test worker] o.a.zookeeper.server.RequestThrottler    : zookeeper.request_throttler.shutdownTimeout = 10000
2022-07-18 15:52:03.158  INFO 13281 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9095
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:59528
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-18 15:52:03.172  INFO 13281 --- [    Test worker] org.apache.zookeeper.common.X509Util     : Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-07-18 15:52:03.243  INFO 13281 --- [    Test worker] kafka.server.KafkaServer                 : starting
2022-07-18 15:52:03.244  INFO 13281 --- [    Test worker] kafka.server.KafkaServer                 : Connecting to zookeeper on 127.0.0.1:59528
2022-07-18 15:52:03.254  INFO 13281 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:59528.
2022-07-18 15:52:03.256  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2022-07-18 15:52:03.257  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:host.name=192.168.0.10
2022-07-18 15:52:03.257  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.version=17.0.3
2022-07-18 15:52:03.257  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.vendor=Amazon.com Inc.
2022-07-18 15:52:03.257  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.home=/Users/myunghan/Library/Java/JavaVirtualMachines/corretto-17.0.3/Contents/Home
2022-07-18 15:52:03.257  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.class.path=/Users/myunghan/.gradle/caches/7.4.1/workerMain/gradle-worker.jar:/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/classes/java/test:/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/classes/java/main:/Users/myunghan/kafka_workspace/kafka-spring-admin-1/build/resources/main:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/2.7.1/28ba2633ff51cabd9792ead85cbbffecdbf9f22e/spring-boot-starter-test-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/2.7.1/48f7e04459ccc16d3532bfc486c1b6d629e6e0fc/spring-boot-starter-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/2.8.7/b767af6592e93e0b1a8e34a405f820c778dc7bcf/spring-kafka-2.8.7.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/2.8.7/32bc4c1071fc8bf649d06319afcd3c4d0058e517/spring-kafka-test-2.8.7.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/2.7.1/5b2bed142ba0354200825673413960404a1914e4/spring-boot-test-autoconfigure-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/2.7.1/923ad789b004e8cc17d67853b1e4d3db11946f0/spring-boot-autoconfigure-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/2.7.1/ad38d8a7caa40a499168b2c2e6063fe9b1656373/spring-boot-test-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/2.7.1/8e49b8e7e9ea470a7772f489532264732ab206a2/spring-boot-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/2.7.1/461cf82dc10505f47d3ce2146bd01721177cde4a/spring-boot-starter-logging-2.7.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/1.3.5/59eb84ee0d616332ff44aba065f3888cf002cd2d/jakarta.annotation-api-1.3.5.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/5.3.21/fe371c85f02b8c6690fc3b3d0950ef4f965db0cd/spring-context-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/5.3.21/2012d940f72c8d3ccb97e8bc99e642f522659374/spring-messaging-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/5.3.21/13f4f564024d2f85502c151942307c3ca851a4f7/spring-tx-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/5.3.21/77f04111344acf6ae422aacacb279819ccef4d70/spring-test-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/5.3.21/58ec4ff7a0ce30a1e2612f04ad0fb13ea806705/spring-aop-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/5.3.21/e3eae7e6d211381642a0b7507a5215e3ac1b32e1/spring-beans-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/5.3.21/ca8c5822fc528066ec717f1e74160a1575c43192/spring-expression-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/5.3.21/1b0c9be6b972e4c615f175c70fc32e80557e68e8/spring-core-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.30/8fde7fe2586328ac3c68db92045e1c8759125000/snakeyaml-1.30.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.3.3/13a6f4edb1f5a8956ec6aa867757e325bc98eee7/spring-retry-1.3.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.1/3b713a205faca52c8e90ac0191df8f1d32be00e2/kafka_2.13-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.1/cf608d26e9ab86100d4bbef207828aa919e5de5a/kafka_2.13-3.1.1-test.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.1.1/64f32bf8cdf4ea34a7692d11ad1848932f008044/kafka-metadata-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.1.1/523510a99ecc1e1f722eaade9e0f9913d641aded/kafka-streams-test-utils-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.1.1/c9fcde2d63ba3f5cac7e4d529706c33799899a7a/kafka-raft-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.1.1/cdda958a7405af8fd2bbcce0b0b6f09a7aaa5ade/kafka-storage-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.1.1/d95f73a26a7a864bfabd9c350c43e92cb7347594/kafka-server-common-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.1.1/6ecc0a3857815027e053662d6bf1e765ef63464c/kafka-streams-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.1.1/8f352621795f711c5073aece6153548859ea6092/kafka-storage-api-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.1/6fb84c57d8c4a87b3fc1e834e2500a36c048afa3/kafka-clients-3.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.1/b63bf4f2387f3b37e5a7fc86958343f52c4aac15/kafka-clients-3.1.1-test.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.google.code.findbugs/jsr305/3.0.2/25ea2e8b0c338a877313bd4672d3fe056ea78f0d/jsr305-3.0.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.7.0/f9d7d9659f2694e61142046ff8a216c047f263e8/json-path-2.7.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/2.3.3/48e3b9cfc10752fba3521d6511f4165bea951801/jakarta.xml.bind-api-2.3.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.22.0/c300c0c6a24559f35fa0bd3a5472dc1edcd0111e/assertj-core-3.22.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/4.5.1/f81fb60bd69b3a6e5537ae23b883326f01632a61/mockito-junit-jupiter-4.5.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.8.2/ddeafe92fc263f895bfb73ffeca7fd56e23c2cce/junit-jupiter-params-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.8.2/c598b4328d2f397194d11df3b1648d68d7d990e3/junit-jupiter-engine-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.8.2/4c21029217adf07e4c0d0c5e192b6bf610c94bdc/junit-jupiter-api-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.8.2/b737de09f19864bd136805c84df7999a142fec29/junit-platform-engine-1.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.8.2/32c8b8617c1342376fd5af2053da6410d8866861/junit-platform-commons-1.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.8.2/5a817b1e63f1217e5c586090c45e681281f097ad/junit-jupiter-5.8.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/4.5.1/ed456e623e5afc6f4cee3ae58144e5c45f3b3bf/mockito-core-4.5.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.0/6c9d5fe2f59da598d9aefc1cfc6528ff3cf32df3/jsonassert-1.5.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.9.0/8959725d90eecfee28acd7110e2bb8460285d876/xmlunit-core-2.9.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.6.3/a6e74f826db85ff8c51c15ef0fa2ea0b462aef25/zookeeper-3.6.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.11/4741689214e9d1e8408b206506cbe76d1c6a7d60/logback-classic-1.2.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.17.2/17dd0fae2747d9a28c67bc9534108823d2376b46/log4j-to-slf4j-2.17.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/1.7.36/ed46d81cef9c412a88caef405b58f93a678ff2ca/jul-to-slf4j-1.7.36.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/5.3.21/b41a2888c0e708f9fd12cf9cc0c29cebbcab2e5e/spring-jcl-5.3.21.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.0-4/338d83645fb93afc9e8b38a12d9d16d41d0819b3/zstd-jni-1.5.0-4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.8.4/66f0d56454509f6e36175f2331572e250e04a6cc/snappy-java-1.1.8.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.7.8/34b47db4364d1916c78c3e26e419e8acbff57d80/jose4j-0.7.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.3/d97c3bcdcf6179e110af8ad2a64ca276843395c1/scala-logging_2.13-3.9.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.2.10/4b21499702e50f8be51d8c277ef5bc7d228999ec/metrics-core-4.2.10.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.36/6c62681a2f655b49963a5983b8b0950a6120ae14/slf4j-api-1.7.36.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.4.8/7c62f5f72ab05eb54d40e2abf0360a2fe9ea477f/json-smart-2.4.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/1.2.2/99f53adba383cb1bf7c3862844488574b559621f/jakarta.activation-api-1.2.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.12.11/ebeebf10ea48f997da583cdb6ce861eee279a507/byte-buddy-1.12.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.12.11/78439af17fa17e4336242d1c4a6aa36282c89139/byte-buddy-agent-1.12.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.2/7fadf57620c8b8abdf7519533e5527367cb51f09/objenesis-3.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.6.3/8990d19ec3db01f45f82d4011a11b085db66de05/zookeeper-jute-3.6.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.5.0/55762d3191a8d6610ef46d11e8cb70c7667342a3/audience-annotations-0.5.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.78.Final/7e902b6018378bb700ec9364b1d0fba6eefd99fd/netty-handler-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.78.Final/dcb28ed02abf7bf02aa8dc9bf05f58bedcb12e9a/netty-transport-native-epoll-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.13.3/ad638e951639b8f5eb0c08e5d3c17abdc103e98e/jackson-dataformat-csv-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.13.3/4ad8fb9d69de4a7010ddb16825d6a9de57b2027c/jackson-module-scala_2.13-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.13.3/7198b3aac15285a49e218e08441c5f70af00fc51/jackson-annotations-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.13.3/a27014716e4421684416e5fa83d896ddb87002da/jackson-core-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.13.3/d4884595d5aab5babdb00ddbd693b8fd36b5ec3c/jackson-datatype-jdk8-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.13.3/56deb9ea2c93a7a556b3afbedd616d342963464e/jackson-databind-2.13.3.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.4.4/9cb262981ba1fac1f25c0e7e2b285d536ec8923b/scala-collection-compat_2.13-2.4.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.0/8ffac68615b438933fe27506e755d790a68b8bab/scala-java8-compat_2.13-1.0.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.6/bb523e56c63746a5752dece28fcd702c54fd3a11/scala-reflect-2.13.6.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.6/ed7a2f528c7389ea65746c22a01031613d98ab3d/scala-library-2.13.6.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.2.0/28c11eb91f9b6d8e200631d46e20a7f407f2a046/opentest4j-1.2.0.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.2.11/a01230df5ca5c34540cdaa3ad5efb012f1f1f792/logback-core-1.2.11.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.17.2/f42d6afa111b4dec5d2aea0fe2197240749a4ea6/log4j-api-2.17.2.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.4.8/6e1bee5a530caba91893604d6ab41d0edcecca9a/accessors-smart-2.4.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.78.Final/2151893a521c6362858071053fddbd270c1f20bd/netty-transport-classes-epoll-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.78.Final/62c64e182a11e31ad80e68a94f0b02b45344df23/netty-transport-native-unix-common-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.78.Final/e8486c8923fc0914df4562a8e0923462e885f88a/netty-codec-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.78.Final/b1639d431e43622d6cbfdd45c30d3fb810fa9101/netty-transport-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.78.Final/691170d70a979757d50f60e16121ced5f411a9b8/netty-resolver-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.78.Final/6c0c2d805a2c8ea30223677d8219235f9ec14c38/netty-buffer-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.78.Final/d6f560098050f88ba11750aa856edd955e4a7707/netty-common-4.1.78.Final.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/6.22.1.1/c9cdf28e714bc93a3e7b6c57d583d3508568a606/rocksdbjni-6.22.1.1.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/myunghan/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.1/a99500cf6eea30535eeac6be73899d048f8d12a8/asm-9.1.jar
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.library.path=/Users/myunghan/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.io.tmpdir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.compiler=&lt;NA&gt;
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.name=Mac OS X
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.arch=x86_64
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.version=12.4
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.name=myunghan
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.home=/Users/myunghan
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.dir=/Users/myunghan/kafka_workspace/kafka-spring-admin-1
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.free=19MB
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.max=512MB
2022-07-18 15:52:03.258  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.total=57MB
2022-07-18 15:52:03.261  INFO 13281 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Initiating client connection, connectString=127.0.0.1:59528 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@6b2e46af
2022-07-18 15:52:03.264  INFO 13281 --- [    Test worker] org.apache.zookeeper.ClientCnxnSocket    : jute.maxbuffer value is 4194304 Bytes
2022-07-18 15:52:03.267  INFO 13281 --- [    Test worker] org.apache.zookeeper.ClientCnxn          : zookeeper.request.timeout value is 0. feature enabled=false
2022-07-18 15:52:03.269  INFO 13281 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Waiting until connected.
2022-07-18 15:52:03.270  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Opening socket connection to server localhost/127.0.0.1:59528.
2022-07-18 15:52:03.270  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : SASL config status: Will not attempt to authenticate using SASL (unknown error)
2022-07-18 15:52:03.271  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Socket connection established, initiating session, client: /127.0.0.1:59529, server: localhost/127.0.0.1:59528
2022-07-18 15:52:03.276  INFO 13281 --- [   SyncThread:0] o.a.z.server.persistence.FileTxnLog      : Creating new log file: log.1
2022-07-18 15:52:03.279  INFO 13281 --- [   SyncThread:0] o.a.zookeeper.audit.ZKAuditProvider      : ZooKeeper audit is disabled.
2022-07-18 15:52:03.281  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Session establishment complete on server localhost/127.0.0.1:59528, session id = 0x100279f3bb90000, negotiated timeout = 16000
2022-07-18 15:52:03.283  INFO 13281 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Connected.
2022-07-18 15:52:03.343  INFO 13281 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Starting
2022-07-18 15:52:03.350  INFO 13281 --- [-process-thread] k.server.FinalizedFeatureChangeListener  : Feature ZK node at path: /feature does not exist
2022-07-18 15:52:03.351  INFO 13281 --- [-process-thread] kafka.server.FinalizedFeatureCache       : Cleared cache
2022-07-18 15:52:03.464  INFO 13281 --- [    Test worker] kafka.server.KafkaServer                 : Cluster ID = F-TtVF8XQEaV1DYQyTcBZQ
2022-07-18 15:52:03.466  WARN 13281 --- [    Test worker] kafka.server.BrokerMetadataCheckpoint    : No meta.properties file under dir /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/meta.properties
2022-07-18 15:52:03.490  INFO 13281 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9095
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:59528
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-18 15:52:03.496  INFO 13281 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9095
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:59528
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-18 15:52:03.518  INFO 13281 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Starting
2022-07-18 15:52:03.518  INFO 13281 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Starting
2022-07-18 15:52:03.519  INFO 13281 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Starting
2022-07-18 15:52:03.520  INFO 13281 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Starting
2022-07-18 15:52:03.537  INFO 13281 --- [    Test worker] kafka.log.LogManager                     : Loading logs from log dirs ArraySeq(/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079)
2022-07-18 15:52:03.541  INFO 13281 --- [    Test worker] kafka.log.LogManager                     : Attempting recovery for all logs in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079 since no clean shutdown file was found
2022-07-18 15:52:03.548  INFO 13281 --- [    Test worker] kafka.log.LogManager                     : Loaded 0 logs in 11ms.
2022-07-18 15:52:03.548  INFO 13281 --- [    Test worker] kafka.log.LogManager                     : Starting log cleanup with a period of 300000 ms.
2022-07-18 15:52:03.550  INFO 13281 --- [    Test worker] kafka.log.LogManager                     : Starting log flusher with a default period of 9223372036854775807 ms.
2022-07-18 15:52:03.558  INFO 13281 --- [    Test worker] kafka.log.LogCleaner                     : Starting the log cleaner
2022-07-18 15:52:03.565  INFO 13281 --- [leaner-thread-0] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Starting
2022-07-18 15:52:03.716  INFO 13281 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2022-07-18 15:52:03.818  INFO 13281 --- [    Test worker] kafka.network.ConnectionQuotas           : Updated connection-accept-rate max connection creation rate to 2147483647
2022-07-18 15:52:03.820  INFO 13281 --- [    Test worker] kafka.network.Acceptor                   : Awaiting socket connections on localhost:9095.
2022-07-18 15:52:03.840  INFO 13281 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2022-07-18 15:52:03.846  INFO 13281 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2022-07-18 15:52:03.855  INFO 13281 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Starting
2022-07-18 15:52:03.856  INFO 13281 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Starting
2022-07-18 15:52:03.857  INFO 13281 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Starting
2022-07-18 15:52:03.857  INFO 13281 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Starting
2022-07-18 15:52:03.868  INFO 13281 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Starting
2022-07-18 15:52:03.882  INFO 13281 --- [    Test worker] kafka.zk.KafkaZkClient                   : Creating /brokers/ids/0 (is it secure? false)
2022-07-18 15:52:03.892  INFO 13281 --- [    Test worker] kafka.zk.KafkaZkClient                   : Stat of the created znode at /brokers/ids/0 is: 25,25,1658127123888,1658127123888,1,0,0,72101158893191168,202,0,25

2022-07-18 15:52:03.893  INFO 13281 --- [    Test worker] kafka.zk.KafkaZkClient                   : Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:9095, czxid (broker epoch): 25
2022-07-18 15:52:03.926  INFO 13281 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Starting
2022-07-18 15:52:03.928  INFO 13281 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Starting
2022-07-18 15:52:03.932  INFO 13281 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Starting
2022-07-18 15:52:03.933  INFO 13281 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Starting
2022-07-18 15:52:03.937  INFO 13281 --- [er-event-thread] kafka.zk.KafkaZkClient                   : Successfully created /controller_epoch with initial epoch 0
2022-07-18 15:52:03.942  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2022-07-18 15:52:03.942  INFO 13281 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Starting up.
2022-07-18 15:52:03.944  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{})
2022-07-18 15:52:03.946  INFO 13281 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Startup complete.
2022-07-18 15:52:03.946  INFO 13281 --- [ker-EventThread] k.server.FinalizedFeatureChangeListener  : Feature ZK node created at path: /feature
2022-07-18 15:52:03.957  INFO 13281 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Starting up.
2022-07-18 15:52:03.960  INFO 13281 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Starting
2022-07-18 15:52:03.960  INFO 13281 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Startup complete.
2022-07-18 15:52:03.968  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Registering handlers
2022-07-18 15:52:03.968  INFO 13281 --- [-process-thread] kafka.server.FinalizedFeatureCache       : Updated cache from existing &lt;empty&gt; to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2022-07-18 15:52:03.970  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting log dir event notifications
2022-07-18 15:52:03.973  INFO 13281 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Starting
2022-07-18 15:52:03.973  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting isr change notifications
2022-07-18 15:52:03.975  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing controller context
2022-07-18 15:52:03.984  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initialized broker epochs cache: HashMap(0 -&gt; 25)
2022-07-18 15:52:03.985  INFO 13281 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Starting
2022-07-18 15:52:03.996  INFO 13281 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2022-07-18 15:52:03.999  INFO 13281 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Starting
2022-07-18 15:52:04.000  INFO 13281 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2022-07-18 15:52:04.000  INFO 13281 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2022-07-18 15:52:04.001  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently active brokers in the cluster: Set(0)
2022-07-18 15:52:04.001  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2022-07-18 15:52:04.002  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.1
2022-07-18 15:52:04.002  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 97671528ba54a138
2022-07-18 15:52:04.002  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1658127124000
2022-07-18 15:52:04.002  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Current list of topics in the cluster: HashSet()
2022-07-18 15:52:04.002  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Fetching topic deletions in progress
2022-07-18 15:52:04.003  INFO 13281 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] started
2022-07-18 15:52:04.004  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics to be deleted: 
2022-07-18 15:52:04.004  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics ineligible for deletion: 
2022-07-18 15:52:04.005  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing topic deletion manager
2022-07-18 15:52:04.005  INFO 13281 --- [er-event-thread] kafka.controller.TopicDeletionManager    : [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2022-07-18 15:52:04.006  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Sending update metadata request
2022-07-18 15:52:04.006  INFO 13281 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [127.0.0.1:9095]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-18 15:52:04.009  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2022-07-18 15:52:04.014  INFO 13281 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Initializing replica state
2022-07-18 15:52:04.015  INFO 13281 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2022-07-18 15:52:04.019  INFO 13281 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2022-07-18 15:52:04.019  INFO 13281 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Controller 0 connected to localhost:9095 (id: 0 rack: null) for sending state change requests
2022-07-18 15:52:04.019  INFO 13281 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Initializing partition state
2022-07-18 15:52:04.020  INFO 13281 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Triggering online partition state changes
2022-07-18 15:52:04.022  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Ready to serve as the new controller with epoch 1
2022-07-18 15:52:04.023  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.1
2022-07-18 15:52:04.023  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 97671528ba54a138
2022-07-18 15:52:04.023  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1658127124023
2022-07-18 15:52:04.026  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions undergoing preferred replica election: 
2022-07-18 15:52:04.027  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions that completed preferred replica election: 
2022-07-18 15:52:04.027  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2022-07-18 15:52:04.027  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Resuming preferred replica election for partitions: 
2022-07-18 15:52:04.028  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2022-07-18 15:52:04.034  INFO 13281 --- [| adminclient-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-1 unregistered
2022-07-18 15:52:04.034  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting the controller scheduler
2022-07-18 15:52:04.036  INFO 13281 --- [| adminclient-1] o.a.k.c.a.i.AdminMetadataManager         : [AdminClient clientId=adminclient-1] Metadata update failed

org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata

2022-07-18 15:52:04.036  INFO 13281 --- [| adminclient-1] o.a.k.clients.admin.KafkaAdminClient     : [AdminClient clientId=adminclient-1] Timed out 1 remaining operation(s) during close.
2022-07-18 15:52:04.038  INFO 13281 --- [| adminclient-1] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2022-07-18 15:52:04.039  INFO 13281 --- [| adminclient-1] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-18 15:52:04.039  INFO 13281 --- [| adminclient-1] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2022-07-18 15:52:04.047  INFO 13281 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:9095 (id: 0 rack: null)
2022-07-18 15:52:04.052  INFO 13281 --- [    Test worker] com.example.kafkaspringadmin1.AppTest    : Starting AppTest using Java 17.0.3 on myunghanui-iMac.local with PID 13281 (started by myunghan in /Users/myunghan/kafka_workspace/kafka-spring-admin-1)
2022-07-18 15:52:04.052  INFO 13281 --- [    Test worker] com.example.kafkaspringadmin1.AppTest    : No active profile set, falling back to 1 default profile: &quot;default&quot;
2022-07-18 15:52:04.124  INFO 13281 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:9095 (id: 0 rack: null)
2022-07-18 15:52:04.647  INFO 13281 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:9095]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-18 15:52:04.650  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.1
2022-07-18 15:52:04.650  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 97671528ba54a138
2022-07-18 15:52:04.650  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1658127124650
2022-07-18 15:52:04.703  INFO 13281 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic thing7 with configuration {} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0))
2022-07-18 15:52:04.718  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(thing7)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(thing7,Some(5qHtdjDSR2-8d0ng2BYMMQ),Map(thing7-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing7-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2022-07-18 15:52:04.719  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for thing7-0,thing7-1
2022-07-18 15:52:04.721  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing7-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.722  INFO 13281 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic thing6 with configuration {} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0), 2 -&gt; ArrayBuffer(0))
2022-07-18 15:52:04.722  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing7-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.722  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.725  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.731  INFO 13281 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic thing2 with configuration {compression.type=zstd} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0), 2 -&gt; ArrayBuffer(0), 3 -&gt; ArrayBuffer(0), 4 -&gt; ArrayBuffer(0), 5 -&gt; ArrayBuffer(0), 6 -&gt; ArrayBuffer(0), 7 -&gt; ArrayBuffer(0), 8 -&gt; ArrayBuffer(0), 9 -&gt; ArrayBuffer(0))
2022-07-18 15:52:04.741  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing7-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.741  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing7-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.743  INFO 13281 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic thing5 with configuration {} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0))
2022-07-18 15:52:04.744  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions
2022-07-18 15:52:04.745  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 2 partitions
2022-07-18 15:52:04.746  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.750  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(thing2, thing5, thing6)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(thing2,Some(3THniU6VRZmiPoXBYqsOAw),HashMap(thing2-4 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-7 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-5 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-6 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-9 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-3 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-2 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing2-8 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))), TopicIdReplicaAssignment(thing5,Some(G4LRJdxNQAy135-n7CMZLQ),Map(thing5-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing5-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))), TopicIdReplicaAssignment(thing6,Some(YDRT2T0GRR2JGMwo--t5_w),Map(thing6-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing6-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing6-2 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2022-07-18 15:52:04.750  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions
2022-07-18 15:52:04.750  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for thing2-4,thing6-1,thing6-2,thing5-1,thing2-7,thing2-1,thing2-5,thing6-0,thing5-0,thing2-6,thing2-9,thing2-0,thing2-3,thing2-2,thing2-8
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing6-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing6-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing5-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-7 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-5 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing6-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing5-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-6 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-9 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-8 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.751  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.752  INFO 13281 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic thing4 with configuration {compression.type=zstd} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0), 2 -&gt; ArrayBuffer(0))
2022-07-18 15:52:04.752  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.766  INFO 13281 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic thing1 with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0), 2 -&gt; ArrayBuffer(0), 3 -&gt; ArrayBuffer(0), 4 -&gt; ArrayBuffer(0), 5 -&gt; ArrayBuffer(0), 6 -&gt; ArrayBuffer(0), 7 -&gt; ArrayBuffer(0), 8 -&gt; ArrayBuffer(0), 9 -&gt; ArrayBuffer(0))
2022-07-18 15:52:04.770  INFO 13281 --- [quest-handler-5] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(thing7-1, thing7-0)
2022-07-18 15:52:04.772  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
2022-07-18 15:52:04.774  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing6-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing6-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing5-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing6-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing5-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing2-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 15 become-leader and 0 become-follower partitions
2022-07-18 15:52:04.775  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 15 partitions
2022-07-18 15:52:04.781  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.794  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [HashSet(thing1, thing4)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(thing1,Some(HdlCILtwQ5WwfOk53pRwQQ),HashMap(thing1-9 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-3 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-8 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-5 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-2 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-7 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-6 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing1-4 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))), TopicIdReplicaAssignment(thing4,Some(mKp-DwG5QOyNxsU7uBDH3w),Map(thing4-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing4-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), thing4-2 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for thing1-9,thing4-2,thing1-3,thing1-8,thing1-5,thing1-2,thing1-7,thing4-1,thing1-1,thing4-0,thing1-6,thing1-0,thing1-4
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-9 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing4-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-8 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-5 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-7 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing4-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing4-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-6 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2022-07-18 15:52:04.796  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.797  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing4-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing4-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing4-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.813  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition thing1-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2022-07-18 15:52:04.814  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 13 become-leader and 0 become-follower partitions
2022-07-18 15:52:04.814  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 13 partitions
2022-07-18 15:52:04.814  INFO 13281 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2022-07-18 15:52:04.833  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing7-1, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.843  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing7-1 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing7-1 with properties {}
2022-07-18 15:52:04.844  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing7-1 broker=0] No checkpointed highwatermark is found for partition thing7-1
2022-07-18 15:52:04.845  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing7-1 broker=0] Log loaded for partition thing7-1 with initial high watermark 0
2022-07-18 15:52:04.846  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing7-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.856  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing7-0, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.857  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing7-0 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing7-0 with properties {}
2022-07-18 15:52:04.857  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing7-0 broker=0] No checkpointed highwatermark is found for partition thing7-0
2022-07-18 15:52:04.857  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing7-0 broker=0] Log loaded for partition thing7-0 with initial high watermark 0
2022-07-18 15:52:04.857  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing7-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.868  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 120ms correlationId 1 from controller 0 for 2 partitions
2022-07-18 15:52:04.874  INFO 13281 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2022-07-18 15:52:04.876  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 15 partitions
2022-07-18 15:52:04.882  INFO 13281 --- [quest-handler-7] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(thing2-4, thing6-1, thing6-2, thing5-1, thing2-7, thing2-1, thing2-5, thing6-0, thing5-0, thing2-6, thing2-9, thing2-0, thing2-3, thing2-2, thing2-8)
2022-07-18 15:52:04.882  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 15 partitions
2022-07-18 15:52:04.886  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-9, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.886  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-9 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-9 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.887  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-9 broker=0] No checkpointed highwatermark is found for partition thing2-9
2022-07-18 15:52:04.887  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-9 broker=0] Log loaded for partition thing2-9 with initial high watermark 0
2022-07-18 15:52:04.887  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.897  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-5, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.897  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-5 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-5 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.897  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-5 broker=0] No checkpointed highwatermark is found for partition thing2-5
2022-07-18 15:52:04.898  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-5 broker=0] Log loaded for partition thing2-5 with initial high watermark 0
2022-07-18 15:52:04.898  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.909  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing6-1, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.909  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing6-1 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing6-1 with properties {}
2022-07-18 15:52:04.909  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing6-1 broker=0] No checkpointed highwatermark is found for partition thing6-1
2022-07-18 15:52:04.909  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing6-1 broker=0] Log loaded for partition thing6-1 with initial high watermark 0
2022-07-18 15:52:04.910  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing6-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.919  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-7, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.920  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-7 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-7 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.920  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-7 broker=0] No checkpointed highwatermark is found for partition thing2-7
2022-07-18 15:52:04.920  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-7 broker=0] Log loaded for partition thing2-7 with initial high watermark 0
2022-07-18 15:52:04.920  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.929  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-1, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.930  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-1 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-1 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.930  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-1 broker=0] No checkpointed highwatermark is found for partition thing2-1
2022-07-18 15:52:04.930  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-1 broker=0] Log loaded for partition thing2-1 with initial high watermark 0
2022-07-18 15:52:04.930  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.940  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing5-1, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.940  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing5-1 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing5-1 with properties {}
2022-07-18 15:52:04.940  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing5-1 broker=0] No checkpointed highwatermark is found for partition thing5-1
2022-07-18 15:52:04.940  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing5-1 broker=0] Log loaded for partition thing5-1 with initial high watermark 0
2022-07-18 15:52:04.941  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing5-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.949  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-3, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.950  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-3 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-3 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.950  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-3 broker=0] No checkpointed highwatermark is found for partition thing2-3
2022-07-18 15:52:04.950  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-3 broker=0] Log loaded for partition thing2-3 with initial high watermark 0
2022-07-18 15:52:04.950  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.959  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-6, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.960  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-6 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-6 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.960  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-6 broker=0] No checkpointed highwatermark is found for partition thing2-6
2022-07-18 15:52:04.960  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-6 broker=0] Log loaded for partition thing2-6 with initial high watermark 0
2022-07-18 15:52:04.960  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.968  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing6-2, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.969  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing6-2 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing6-2 with properties {}
2022-07-18 15:52:04.969  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing6-2 broker=0] No checkpointed highwatermark is found for partition thing6-2
2022-07-18 15:52:04.969  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing6-2 broker=0] Log loaded for partition thing6-2 with initial high watermark 0
2022-07-18 15:52:04.969  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing6-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.977  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-8, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.978  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-8 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-8 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.978  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-8 broker=0] No checkpointed highwatermark is found for partition thing2-8
2022-07-18 15:52:04.978  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-8 broker=0] Log loaded for partition thing2-8 with initial high watermark 0
2022-07-18 15:52:04.978  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.988  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-2, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.988  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-2 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-2 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.988  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-2 broker=0] No checkpointed highwatermark is found for partition thing2-2
2022-07-18 15:52:04.988  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-2 broker=0] Log loaded for partition thing2-2 with initial high watermark 0
2022-07-18 15:52:04.988  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:04.997  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-4, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:04.998  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-4 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-4 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:04.998  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-4 broker=0] No checkpointed highwatermark is found for partition thing2-4
2022-07-18 15:52:04.998  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-4 broker=0] Log loaded for partition thing2-4 with initial high watermark 0
2022-07-18 15:52:04.998  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.007  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing6-0, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.007  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing6-0 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing6-0 with properties {}
2022-07-18 15:52:05.007  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing6-0 broker=0] No checkpointed highwatermark is found for partition thing6-0
2022-07-18 15:52:05.007  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing6-0 broker=0] Log loaded for partition thing6-0 with initial high watermark 0
2022-07-18 15:52:05.007  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing6-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.016  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing5-0, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.016  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing5-0 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing5-0 with properties {}
2022-07-18 15:52:05.016  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing5-0 broker=0] No checkpointed highwatermark is found for partition thing5-0
2022-07-18 15:52:05.017  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing5-0 broker=0] Log loaded for partition thing5-0 with initial high watermark 0
2022-07-18 15:52:05.017  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing5-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.025  INFO 13281 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=thing2-0, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.026  INFO 13281 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition thing2-0 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing2-0 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:05.026  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-0 broker=0] No checkpointed highwatermark is found for partition thing2-0
2022-07-18 15:52:05.026  INFO 13281 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition thing2-0 broker=0] Log loaded for partition thing2-0 with initial high watermark 0
2022-07-18 15:52:05.026  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader thing2-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.033  INFO 13281 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 156ms correlationId 3 from controller 0 for 15 partitions
2022-07-18 15:52:05.034  INFO 13281 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Add 15 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2022-07-18 15:52:05.036  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 13 partitions
2022-07-18 15:52:05.040  INFO 13281 --- [quest-handler-5] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(thing1-9, thing4-2, thing1-3, thing1-8, thing1-5, thing1-2, thing1-7, thing4-1, thing1-1, thing4-0, thing1-6, thing1-0, thing1-4)
2022-07-18 15:52:05.040  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 13 partitions
2022-07-18 15:52:05.043  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-7, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.044  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-7 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-7 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.044  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-7 broker=0] No checkpointed highwatermark is found for partition thing1-7
2022-07-18 15:52:05.044  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-7 broker=0] Log loaded for partition thing1-7 with initial high watermark 0
2022-07-18 15:52:05.044  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.053  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-9, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.054  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-9 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-9 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.054  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-9 broker=0] No checkpointed highwatermark is found for partition thing1-9
2022-07-18 15:52:05.054  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-9 broker=0] Log loaded for partition thing1-9 with initial high watermark 0
2022-07-18 15:52:05.054  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.064  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-3, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.064  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-3 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-3 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.064  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-3 broker=0] No checkpointed highwatermark is found for partition thing1-3
2022-07-18 15:52:05.064  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-3 broker=0] Log loaded for partition thing1-3 with initial high watermark 0
2022-07-18 15:52:05.064  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.073  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-5, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.073  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-5 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-5 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.073  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-5 broker=0] No checkpointed highwatermark is found for partition thing1-5
2022-07-18 15:52:05.073  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-5 broker=0] Log loaded for partition thing1-5 with initial high watermark 0
2022-07-18 15:52:05.073  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.083  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing4-1, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.083  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing4-1 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing4-1 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:05.083  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing4-1 broker=0] No checkpointed highwatermark is found for partition thing4-1
2022-07-18 15:52:05.083  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing4-1 broker=0] Log loaded for partition thing4-1 with initial high watermark 0
2022-07-18 15:52:05.084  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing4-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.092  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-1, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.093  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-1 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-1 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.093  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-1 broker=0] No checkpointed highwatermark is found for partition thing1-1
2022-07-18 15:52:05.093  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-1 broker=0] Log loaded for partition thing1-1 with initial high watermark 0
2022-07-18 15:52:05.093  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.102  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-6, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.103  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-6 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-6 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.103  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-6 broker=0] No checkpointed highwatermark is found for partition thing1-6
2022-07-18 15:52:05.103  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-6 broker=0] Log loaded for partition thing1-6 with initial high watermark 0
2022-07-18 15:52:05.103  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.112  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-8, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.112  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-8 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-8 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.112  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-8 broker=0] No checkpointed highwatermark is found for partition thing1-8
2022-07-18 15:52:05.112  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-8 broker=0] Log loaded for partition thing1-8 with initial high watermark 0
2022-07-18 15:52:05.112  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.123  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing4-0, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.124  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing4-0 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing4-0 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:05.124  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing4-0 broker=0] No checkpointed highwatermark is found for partition thing4-0
2022-07-18 15:52:05.124  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing4-0 broker=0] Log loaded for partition thing4-0 with initial high watermark 0
2022-07-18 15:52:05.124  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing4-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.134  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-2, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.134  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-2 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-2 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.134  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-2 broker=0] No checkpointed highwatermark is found for partition thing1-2
2022-07-18 15:52:05.134  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-2 broker=0] Log loaded for partition thing1-2 with initial high watermark 0
2022-07-18 15:52:05.134  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.143  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing4-2, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.144  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing4-2 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing4-2 with properties {compression.type=&quot;zstd&quot;}
2022-07-18 15:52:05.144  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing4-2 broker=0] No checkpointed highwatermark is found for partition thing4-2
2022-07-18 15:52:05.144  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing4-2 broker=0] Log loaded for partition thing4-2 with initial high watermark 0
2022-07-18 15:52:05.144  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing4-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.153  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-4, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.154  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-4 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-4 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.154  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-4 broker=0] No checkpointed highwatermark is found for partition thing1-4
2022-07-18 15:52:05.154  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-4 broker=0] Log loaded for partition thing1-4 with initial high watermark 0
2022-07-18 15:52:05.154  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.163  INFO 13281 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=thing1-0, dir=/var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079] Loading producer state till offset 0 with message format version 2
2022-07-18 15:52:05.163  INFO 13281 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition thing1-0 in /var/folders/sj/frld0myx4n556tdy5xz3y_rw0000gn/T/spring.kafka.a48614c8-f350-4667-bf6b-58e913a980dd15251394351062913079/thing1-0 with properties {cleanup.policy=compact}
2022-07-18 15:52:05.163  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-0 broker=0] No checkpointed highwatermark is found for partition thing1-0
2022-07-18 15:52:05.163  INFO 13281 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition thing1-0 broker=0] Log loaded for partition thing1-0 with initial high watermark 0
2022-07-18 15:52:05.163  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader thing1-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2022-07-18 15:52:05.171  INFO 13281 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 135ms correlationId 5 from controller 0 for 13 partitions
2022-07-18 15:52:05.172  INFO 13281 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Add 13 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
2022-07-18 15:52:05.179  INFO 13281 --- [| adminclient-2] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-2 unregistered
2022-07-18 15:52:05.180  INFO 13281 --- [| adminclient-2] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2022-07-18 15:52:05.180  INFO 13281 --- [| adminclient-2] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-18 15:52:05.180  INFO 13281 --- [| adminclient-2] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2022-07-18 15:52:05.189  INFO 13281 --- [    Test worker] com.example.kafkaspringadmin1.AppTest    : Started AppTest in 2.628 seconds (JVM running for 3.479)
2022-07-18 15:52:05.455  INFO 13281 --- [    Test worker] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9095]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-07-18 15:52:05.460  INFO 13281 --- [    Test worker] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-1] Instantiated an idempotent producer.
2022-07-18 15:52:05.470  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.1
2022-07-18 15:52:05.470  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 97671528ba54a138
2022-07-18 15:52:05.470  INFO 13281 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1658127125470
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-0 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-5 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-8 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-2 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-9 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-4 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-1 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-6 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-7 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.479  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition thing1-3 to 0 since the associated topicId changed from null to HdlCILtwQ5WwfOk53pRwQQ
2022-07-18 15:52:05.483  INFO 13281 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: F-TtVF8XQEaV1DYQyTcBZQ
2022-07-18 15:52:05.488  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1
2022-07-18 15:52:05.490  INFO 13281 --- [ad | producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2022-07-18 15:52:17.353  WARN 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Client session timed out, have not heard from server in 11866ms for session id 0x100279f3bb90000
2022-07-18 15:52:17.355  INFO 13281 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Processing automatic preferred replica leader election
2022-07-18 15:52:17.357  WARN 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Session 0x100279f3bb90000 for sever localhost/127.0.0.1:59528, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.

org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 11866ms for session id 0x100279f3bb90000
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1258) ~[zookeeper-3.6.3.jar:3.6.3]

2022-07-18 15:52:19.293  WARN 13281 --- [WorkerThread-10] o.apache.zookeeper.server.NIOServerCnxn  : Unexpected exception

org.apache.zookeeper.server.ServerCnxn$EndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /127.0.0.1:59529, session = 0x100279f3bb90000
	at org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:163) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:326) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154) ~[zookeeper-3.6.3.jar:3.6.3]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[na:na]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:833) ~[na:na]

2022-07-18 15:52:21.363  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Opening socket connection to server localhost/127.0.0.1:59528.
2022-07-18 15:52:21.363  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : SASL config status: Will not attempt to authenticate using SASL (unknown error)
2022-07-18 15:52:21.782  INFO 13281 --- [ SessionTracker] o.a.zookeeper.server.ZooKeeperServer     : Expiring session 0x100279f3bb90000, timeout of 16000ms exceeded
2022-07-18 15:52:21.782  INFO 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Socket connection established, initiating session, client: /127.0.0.1:59537, server: localhost/127.0.0.1:59528
2022-07-18 15:52:24.986  INFO 13281 --- [WorkerThread-11] o.a.zookeeper.server.ZooKeeperServer     : Invalid session 0x100279f3bb90000 for client /127.0.0.1:59537, probably expired
2022-07-18 15:52:24.987  WARN 13281 --- [WorkerThread-11] o.apache.zookeeper.server.NIOServerCnxn  : CancelledKeyException causing close of session: 0x100279f3bb90000
2022-07-18 15:52:24.987  WARN 13281 --- [27.0.0.1:59528)] org.apache.zookeeper.ClientCnxn          : Session 0x100279f3bb90000 for sever localhost/127.0.0.1:59528, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.

org.apache.zookeeper.ClientCnxn$EndOfStreamException: Unable to read additional data from server sessionid 0x100279f3bb90000, likely server has closed socket
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290) ~[zookeeper-3.6.3.jar:3.6.3]

</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 7.4.1</a> at 2022. 7. 18. 오후 3:52:31</p>
</div>
</div>
</body>
</html>
